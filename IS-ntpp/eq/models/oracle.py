import randomfrom typing import List, Optional, Tuple, Unionimport torchimport numpy as npimport eqfrom eq.data.batch import BatchISfrom eq.data import SequenceISfrom .tpp_model import TPPModelimport eq.distributions as distclass Oracle(TPPModel):    """ Oracle: a neural temporal point process (NTPP) model to fit/forecast induced seismicity sequences.    Args:        input_magnitude: Should magnitude be used as a model input feature?        input_injection: Should injection data be used as a model input feature?        train_to_forecast: Should the model be trained to forecast the seqeunce too?        supplementary_mark_list: List of supplementary marks to use as input features.        log_tau_mean: Mean inter-event times in the dataset.        log_tau_std: Standard deviation of the inter-event times in the dataset.        mag_mean: Mean earthquake magnitude in the dataset.        history_size: Size of the RNN hidden state, or history embedding.        num_dist_components: Number of individual component distributions in the output mixture distribution.        dropout_prob: Dropout probability.        learning_rate: Learning rate used in optimization.        learning_decay_rate: The epoch-rate of decay for the learning rate.        encoder_type: Type of encoder architecture. Possible choices {'GRU', 'RNN', 'LSTM', 'None'}        decoder_type: Type of decoder architecture. Possible choices {'FCN', 'Trans'}    """    def __init__(        self,        input_magnitude: bool = True,        input_injection: bool = True,        train_to_forecast: bool = True,        supplementary_mark_list: Optional[str] = [],        log_tau_mean: float = 0.0,        log_tau_std: float = 2.0,        mag_mean: float = 0.0,        history_size: int = 8, # (Dh)        num_dist_components: int = 32, # (Dt)        dropout_prob: float = 0.2,        learning_rate: float =1e-2,        learning_decay_rate: float = 0.25,        encoder_type = 'GRU',        decoder_type = 'FCN',        ):        super().__init__()        # Initialize.        self.input_injection = input_injection        self.input_magnitude = input_magnitude        self.train_to_forecast = train_to_forecast        self.history_size = history_size        self.num_dist_components = num_dist_components        self.learning_rate = learning_rate        self.learning_decay_rate = learning_decay_rate        self.encoder_type = encoder_type        self.decoder_type = decoder_type        self.register_buffer('log_tau_mean', torch.tensor(log_tau_mean, dtype=torch.float32))        self.register_buffer('log_tau_std', torch.tensor(log_tau_std, dtype=torch.float32))        self.register_buffer('mag_mean', torch.tensor(mag_mean, dtype=torch.float32))        # Handling the supplementary marks.        self.supplementary_mark_list = supplementary_mark_list        self.num_supp_marks = len(supplementary_mark_list)        # Encoder input mark and context sizes.        self.num_marks = ( # (Dm)            2  # Inter-event times & smoothed seismicity rate.            + 2*int(self.input_magnitude)  # Magnitude & sequence marks.            + 4*int(self.input_injection)  # Injection marks.            + self.num_supp_marks # Supplementary marks.            )        self.context_size = self.num_marks + int(self.encoder_type!='None')*self.num_marks # (Dc)        # Decoder output size.        self.num_time_params = 3 * self.num_dist_components # (Dw)        # Indicies of future-knowable marks to pass forward to the decoder for forecasting.        # Note that this assumes injection/sequence marks have been turned on.        PF_idx = [4,6,7,3] # vm, sv, dTS, Mc.        # Encoder setup.        if self.encoder_type != 'None':            # Use a recurrent nerual network to encode the history embedding.            self.rnn = eq.DL.IS_RNN_encoder(                rnn_type = self.encoder_type,                 d_model_in = self.num_marks,                 #d_model_out = self.history_size,                 dropout_prob = dropout_prob,                 )        # Decoder setup.        if self.decoder_type == 'Trans':            # Use a transformer for decoding the mean inter-event time.            self.decoder = eq.DL.IS_Transformer_decoder(                d_modelDs = self.context_size,                 d_modelDt = len(PF_idx),                 d_model_out = self.num_time_params,                 nheadDs = self.num_marks,                 nheadDt = 1,                 dim_feedforward = self.context_size*2,                 dropout_prob = dropout_prob,                 F_idx = PF_idx                )        elif self.decoder_type == 'FCN' :           # Use a fully-connected network for decoding the mean inter-event time.            self.decoder = eq.DL.IS_FCN_decoder(                d_model_in = self.context_size,                 d_model_ff = self.context_size,                 d_model_out = self.num_time_params,                 lookback_size = 1,                 dropout_prob = dropout_prob,                 F_idx = PF_idx                )        self.get_dT_dist = eq.DL.dist_decoder(num_dist_components=self.num_dist_components)    def prep_time_marks(self, inter_times):        # Pre-processing and error handling for the inter-event times.        # inter_times has shape (...), output has shape (..., 1)        log_tau = torch.log10(inter_times.clamp(1e-10,1e+10))        return (log_tau.unsqueeze(-1) - self.log_tau_mean) / self.log_tau_std    def prep_mag_marks(self, mag):        # Pre-processing and error handling for the magnitude marks.        # mag has shape (...), output has shape (..., 1)        mag = mag.clamp(-10, +10)        return mag.unsqueeze(-1) - self.mag_mean    def prep_inj_marks(self, inj_mark):        # Assuming that all injection marks have been adequately pre-processed.        # inj_mark has shape (...), output has shape (..., 1)        inj_mark = inj_mark.clamp(-10, +10)        return inj_mark.unsqueeze(-1)    def prep_supp_marks(self, supp_mark,mark_name):        # Assuming that all supplementary marks have been adequately pre-processed.        # supp_mark has shape (...), output has shape (..., 1)        supp_mark = supp_mark.clamp(-10, +10)        return supp_mark.unsqueeze(-1)    def get_marks(self, batch):        """ Get and prepare the input marks for every event in the batch.        Returns:            marks: The input marks, a tensor with shape (batch_size[B], seq_len[L], mark_size[Dm]).        """        # Create the input feature tensor from all of the flagged marks.        mark_list = [self.prep_time_marks(batch.inter_times)]        mark_list.append(self.prep_supp_marks(batch['aRs'],'aRs'))        if self.input_magnitude:            mark_list.append(self.prep_mag_marks(batch.mag))            Mc=batch.mag_completeness * torch.ones_like(batch.mag)            mark_list.append(self.prep_mag_marks(Mc))        if self.input_injection:            mark_list.append(self.prep_inj_marks(batch.vm))            mark_list.append(self.prep_inj_marks(batch.dVc))            mark_list.append(self.prep_inj_marks(batch.sv))            mark_list.append(self.prep_inj_marks(batch.dTS))        for mark in self.supplementary_mark_list:            mark_list.append(self.prep_supp_marks(batch[mark],mark))        marks = torch.cat(mark_list, dim=-1) # (B, L, Dm)        return marks # (B, L, Dm)    def get_pre_params(self, marks, forecasting=False, idx_split=None, use_injection=False):        """ Estimate the pre-parameters needed for decoding the inter-event time distribution.        Returns:            pre_params: Oracle's estimates of the inter-event time distribution pre-parameters, a tensor with shape (batch_size[B], seq_len[L], num_time_params[Dw]).        """        # Encode the marks into a history embedding and create the context embedding.        if self.encoder_type != 'None':            embedding = self.rnn(marks)[0]  # (B, L, Dh)            embedding = torch.cat([marks,embedding],-1) # (B, L, Dc)        else:            embedding = marks        # Decode the context embedding.        pre_params = self.decoder.forward(embedding, forecasting=forecasting, idx=idx_split) # (B, L, Dw)        # Return the pre-parameters.        return pre_params # (B, L, Dw)    def loss(self, batch: BatchIS, forecasting=False, forecast_count=0, add_rate_misfit=False) -> torch.Tensor:        """ Compute the final negative log-likelihoods (nLL) for a batch of event sequences.        Args:            batch: A batch of padded event sequences.        Returns:            nLL: nLL of each sequence, shape (batch_size,)        """        # Get the marks and pre-params.        marks = self.get_marks(batch)  # (B, L, Dm)        pre_params = self.get_pre_params(marks) # (B, L, Dw)        # Get the dT observations and predictions.        mask = batch.mask[:,1:-1] # (B, L-2)        dTobs = batch.inter_times.clamp(1e-10,1e+10) # (B, L)        dT_dist = self.get_dT_dist(pre_params[:,0:-2,:])  # (B, L-2) -ns        # Get the corresponding log-likelihoods.        log_pdf = dT_dist.log_prob(dTobs[:,1:-1])  # (B, L-2)        log_like = (log_pdf * mask).sum(-1) # (B)        # Survival value from last distribution-estimation at time t_end.        arange = torch.arange(batch.batch_size)        last_dist = self.get_dT_dist(pre_params[arange, batch.end_idx, :]) # (B, 1) -ns        last_log_surv = last_dist.log_survival( dTobs[arange, batch.end_idx] ) # (B, 1)        log_like = log_like + last_log_surv.squeeze(-1)  # (B)        # Adding on a rate-misfit to penalize poorly modelled mean inter-event times.        if add_rate_misfit:            dTpred = dT_dist.mean # (B, L-2)            Rloss = ((dTpred.log10()-dTobs[:,1:-1].log10())*mask).norm(p=1,dim=-1) # (B)            log_like = log_like - Rloss        # Also consider the log-likelihood loss from forecasting, if flagged to.        if forecasting:            # Handling the number of forecast realizations.            Nl=marks.shape[1]            if forecast_count==0:                idx_list = range(1, batch.seq_len-1)                forecast_count = batch.end_idx            else:                idx_list = random.sample(range(1, batch.seq_len-1),forecast_count)            # Loop over each of the forecast realizations.            for i in idx_list:                # Make sure i isn't too big or too small.                if(i==0):                    i = i + 2                elif(i==1):                    i = i + 1                elif(i==(Nl-0)):                    i = i - 3                elif(i==(Nl-1)):                    i = i - 2                elif(i==(Nl-2)):                    i = i - 1                # Compute the individual log-likelihoods for each forecast realizations.                pre_params_f = self.get_pre_params(marks, forecasting=True, idx_split=i) # (B, L-i-1, Dw)                dT_dist_f = self.get_dT_dist(pre_params_f[:,0:-2,:])  # (B, L-i-2) -ns                log_pdf_f = dT_dist_f.log_prob(dTobs[:,i:-2])  # (B, L-i-2)                log_pdf_f = (torch.cat([ log_pdf[:,0:i], log_pdf_f ], dim=-1)*mask).sum(-1) # (B)                log_like = log_like + log_pdf_f                # Survival value from last distribution-estimation at time t_end.                pre_params_f = torch.cat([ pre_params[:,0:i,:], pre_params_f ], dim=1) # (B, L, Dw)                last_dist_f = self.get_dT_dist(pre_params_f[arange, batch.end_idx, :]) # (B, 1) -ns                last_log_surv_f = last_dist_f.log_survival( dTobs[arange, batch.end_idx] ) # (B, 1)                log_like = log_like + last_log_surv_f.squeeze(-1)                # Add a rate-misfit penalty for the forecasted mean inter-event times.                if add_rate_misfit:                    dTpredf = dT_dist_f.mean # (B, L-i-1)                    dTpredf = torch.cat([dTpred[:,0:i],dTpredf[:,0:-1]],dim=1) # (B, L-1)                    Rloss = Rloss + ((dTpredf-dTobs[:,1:-1])*mask).norm(p=1,dim=-1) # (B)            # Consider the average loss amongst all forecast realizations.            log_like = log_like / (forecast_count + 1) # (B)        # Remove survival time from t_prev to t_nll_start        if torch.any(batch.t_nll_start != batch.t_start):            prev_surv_dist = self.get_dT_dist(pre_params[arange, batch.start_idx, :])            prev_surv_time = batch.inter_times[arange, batch.start_idx] - (                batch.arrival_times[arange, batch.start_idx] - batch.t_nll_start            )            prev_log_surv = prev_surv_dist.log_survival(prev_surv_time)            log_like = log_like - prev_log_surv        # Return an event normalized loss.        return -log_like / (batch.end_idx)  # (B)    def evaluate_nll(self, sequence: SequenceIS) -> torch.Tensor:        """ Compute the sequence of cumulative negative log-likelihood (nLL), given an event sequence.        Args:            sequence: Sequence of induced seismicity.        Returns:            nll: The cumulative nLL of the sequence.        """        # Get the marks and pre-params.        batch = BatchIS.from_list([sequence]) # (1, L)        marks = self.get_marks(batch)  # (1, L, Dm)        pre_params = self.get_pre_params(marks) # (1, L, Dw)        # Get the observations and predicted dT distribution.        dTobs = batch.inter_times.clamp(1e-10,1e+10) # (1, L)        dT_dist = self.get_dT_dist(pre_params[:,0:-2,:]) # (1, L-2) -ns        mask = batch.mask[:,1:-1] # (B, L-2)        # Get the corresponding log-likelihoods.        log_pdf = dT_dist.log_prob(dTobs[:,1:-1])  # (1, L-2)        log_like = (log_pdf * mask).cumsum(-1) # (1, L-2)        # Survival value from last distribution-estimation at time t_end.        arange = torch.arange(batch.batch_size)        last_surv_dist = self.get_dT_dist(pre_params[arange, batch.end_idx, :])        last_log_surv = last_surv_dist.log_survival(dTobs[arange, batch.end_idx])        log_like = log_like + last_log_surv.squeeze(-1)  # (1, L-2) FIX        # Remove survival time from t_prev to t_nll_start        if torch.any(batch.t_nll_start != batch.t_start):            prev_surv_dist = self.get_dT_dist(pre_params[arange, batch.start_idx, :])            prev_surv_time = batch.inter_times[arange, batch.start_idx] - (                batch.arrival_times[arange, batch.start_idx] - batch.t_nll_start            )            prev_log_surv = prev_surv_dist.log_survival(prev_surv_time)            log_like = log_like - prev_log_surv        return -log_like.T.reshape(-1)  # (L-2)        def evaluate_intensity(        self,        sequence: SequenceIS,        num_grid_points: int = 10, # Ngp        eps: float = 1e-10,    ) -> Tuple[torch.Tensor, torch.Tensor]:        """ Evaluate the rate of seismicity (i.e., the intensity) for a given sequence.                    Args:            sequence: Sequence of induced seismicity.            num_grid_points: Number of evenly-spaced interpolation points between events to evaluate the intensity on.            eps: Smallest amount of time after an event to to evaluate the intensity on.        Returns:            grid: The (interpolated) arrival times that the intensity was evaluated on.            intensity: The estimated intensity.        """        # Special case handling (evaluate only on the event times).        if num_grid_points==0:            num_grid_points = 1            eps = 1        # Get the dT distribution, from the input sequence.        batch = BatchIS.from_list([sequence]) # (1, L)        marks = self.get_marks(batch)  # (1, L, Dm)        pre_params = self.get_pre_params(marks) # (1, L, Dw)        dT_dist = self.get_dT_dist(pre_params) # (1, L) -ns        # Evaluate each hazard function at times x = [eps, ..., tau_i].        x = batch.inter_times * torch.linspace(eps, 1, num_grid_points)[:, None] # (Ngp, L)        intensity = dT_dist.log_hazard(x).T.reshape(-1).exp() # (Ngp*L, )        # Shift the inter-event times x to get the arrival times.        offsets = torch.cat([torch.tensor(sequence.t_start).unsqueeze(0), sequence.arrival_times])        grid = (x + offsets).T.reshape(-1)        return grid, intensity # (Ngp*L)    def evaluate_compensator(        self,        sequence: SequenceIS,        num_grid_points: int = 10, # Ngp        eps: float = 1e-10,    ) -> Tuple[torch.Tensor, torch.Tensor]:        """Evaluate the cumulative survival function of seismicity (i.e., the compensator) for a given sequence.                    Args:            sequence: Sequence of induced seismicity.            num_grid_points: Number of evenly-spaced interpolation points between events to evaluate the intensity on.            eps: Smallest amount of time after an event to to evaluate the intensity on.        Returns:            grid: The (interpolated) arrival times that the intensity was evaluated on.            compensator: The estimated compensator.        """        # Special case handling (evaluate only on the event times).        if num_grid_points==0:            num_grid_points = 1            eps = 1        # Get the dT distribution, from the input sequence.        batch = BatchIS.from_list([sequence]) # (1, L)        marks = self.get_marks(batch)  # (1, L, Dm)        pre_params = self.get_pre_params(marks) # (1, L, Dw)        dT_dist = self.get_dT_dist(pre_params) # (1 ,L) -ns        # Evaluate each log survival function at times x = [eps, ..., tau_i].        x = batch.inter_times * torch.linspace(eps, 1, num_grid_points)[:, None] # (Ngp, L)        log_surv = dT_dist.log_survival(x) # (Ngp, L)        # Compute the cumulative sum of log survival functions to get the compensator.        surv_offsets = torch.cat(            [torch.tensor([0.0]), log_surv[-1].cumsum(dim=-1)[:-1]]        )        compensator = -(log_surv + surv_offsets).T.reshape(-1) # (Ngp*L)        # Shift the inter-event times x to get the arrival times.        offsets = torch.cat([torch.tensor(sequence.t_start).unsqueeze(0), sequence.arrival_times])        grid = (x + offsets).T.reshape(-1)        return grid, compensator # (Ngp*L)    def forecast_intensity(        self,        sequence: SequenceIS,        t_forecast: float,    ) -> Tuple[torch.Tensor,torch.Tensor, torch.Tensor,torch.Tensor, torch.Tensor]:        """ First evaluate the intensity over the interval from t_start to t_forecast.            Then forecast the intensity for the remaining interval (i.e., from t_forecast to t_end).                Args:            sequence: Sequence of induced seismicity.            t_forecast: the time to split the sequence into fitting/forecasting portions.        Returns:            A tuple of arrival times, fit/forecast intensities, mean arrival times, and mean number of forecast events.        """        # Split the sequence into two sub-sequences.        seq1 = sequence.get_subsequence(sequence.t_start, t_forecast)        seq2 = sequence.get_subsequence(t_forecast, sequence.t_end)        i_split = len(seq1.arrival_times)        # Fit the first sequence.        grid1,intensity1 = self.evaluate_intensity(sequence=seq1,num_grid_points=0)                # Get the dT distribution, from the input sequence.        batch = BatchIS.from_list([sequence]) # (1, L)        marks = self.get_marks(batch)  # (1, L, Dm)        pre_params2 = self.get_pre_params(marks, forecasting=True, idx_split=i_split) # (1, L2, Dw)        dT_dist2 = self.get_dT_dist(pre_params2) # (1, L2) -ns        # Evaluate each hazard function at times x = [eps, ..., tau_i].        mean_dT=dT_dist2.mean.unsqueeze(0).clamp(1e-10,1e+10)        x = mean_dT * torch.linspace(1, 1, 1)[:, None] # (Ngp, L2)        intensity2 = dT_dist2.log_hazard(x).T.reshape(-1).exp() # (Ngp*L2)                # Shift the inter-event times x to get the arrival times.        offsets = torch.cat([torch.tensor(seq2.t_start).unsqueeze(0), seq2.arrival_times])        grid2 = (offsets).T.reshape(-1).T.reshape(-1)        # Return the concatenation of both.        return grid1,grid2, intensity1,intensity2, mean_dT.reshape(-1)    def get_magnitude_dist(self, context, b=1.0, Mc=0.0):        ones = torch.ones_like(context[...,0])  # (B, L)        b = b * ones        Mc = Mc * ones        return dist.GutenbergRichter(b=b, mag_min=Mc)    def sample(        self,        sequence: SequenceIS,        t_start: float,        t_end: float,        batch_size: int = 10,        b: float = 1.0,        return_sequences: bool = False,        update_history: bool = False,        eps: float = 1e-10,    ) -> Union[BatchIS, List[SequenceIS]]:        """ Sample a batch of events from the model.        Args:            sequence: Sequence of induced seismicity.            t_start: Start interval on which to sample the TPP.            t_end: End interval on which to sample the TPP.            batch_size: Number of sequences to generate.            b: Gutenberg-Richter b-value.            return_sequences: If True, returns samples as List[SequenceIS]. If False, returns samples as eq.data.BatchIS.            update_history: If true, iteratively update the history using the sampled sequence.            eps: Small amount of time to ensure past/future sequences don't contain the same event.        Returns:            batch: Batch/sequences generated from the model.        """        # Flagging for use of supplementary marks.        if self.num_supp_marks != 0:            raise ValueError("Sampling is not currently supported for extra features")        if (self.lookback_size>1):            raise ValueError("Sampling does not currently support lookback")        # Get the requisite information from the fit sub-sequence.        past_seq = sequence.get_subsequence(sequence.t_start, t_start)        past_batch = BatchIS.from_list([past_seq])        past_history, current_hidden_state = self.get_history(past_batch) # (1, L, Dh), (num_layers, 1, Dh)        current_hidden_state = current_hidden_state.expand(-1,batch_size,-1) # (num_layers, B, Dh)        # Get the context/history for the last fitted event.        current_context = self.get_context(past_history,past_batch)[:,[-1],:]  # (1, 1, Dc)        current_context = current_context.expand(batch_size,-1,-1) # (B, 1, Dc)        current_history = current_context[...,0:self.history_size] # (B, 1, Dh)        # Compute some other relevant values.        time_remaining = past_seq.t_end - past_seq.arrival_times[-1]        ones = torch.ones_like(current_context[...,0])        t_start = past_seq.arrival_times[-1]        duration = t_end - t_start        samp_seq = sequence.get_subsequence(t_start+eps, sequence.t_end)        # Get the Gutenberg-Richter magnitude-frequency distribution.        Mc = past_seq.mag_completeness        mag_dist = self.get_magnitude_dist(current_context,b,Mc)        # Preallocate.        inter_times = torch.empty(batch_size, 0, device=self.device)        magnitudes = torch.empty(batch_size, 0, device=self.device)        if self.input_injection:            vm = torch.empty(batch_size, 0, device=self.device)            dVc = torch.empty(batch_size, 0, device=self.device)            sv = torch.empty(batch_size, 0, device=self.device)            dTS = torch.empty(batch_size, 0, device=self.device)        else:            vm = None            dVc = None            sv = None            dTS = None        # Loop until each of the batches of sequences are complete.        generated = False        while not generated:            # Sample the time until the next event (batch).            inter_time_dist = self.get_dT_dist(current_context)            if time_remaining is None:                next_inter_times = inter_time_dist.sample()  # (B, 1)            else:                next_inter_times = inter_time_dist.sample_conditional(                    lower_bound=time_remaining                )  # (B, 1)                next_inter_times -= time_remaining                time_remaining = None            next_inter_times.clamp_max_(t_end - t_start)            inter_times = torch.cat([inter_times, next_inter_times], dim=-1)  # (B, L)            # Prepare RNN input and context, given this newly sampled event (batch).            rnn_input_list = [self.prep_time_marks(next_inter_times)]            context_list = []            next_mag = mag_dist.sample()  # (B, 1)            magnitudes = torch.cat([magnitudes, next_mag], dim=-1)  # (B, L)            rnn_input_list.append(self.prep_mag_marks(next_mag))            rnn_input_list.append(self.prep_mag_marks(Mc * ones))            context_list.append(self.prep_mag_marks(Mc * ones))            if self.input_injection:                arrival_times = inter_times.cumsum(-1)[...,[-1]] + t_start # (B, 1)                vm_i,dVc_i,sv_i,dTS_i = self.interpolate_injection(arrival_times,sequence)                vm = torch.cat([vm, vm_i], dim=-1)     # (B, L)                dVc = torch.cat([dVc, dVc_i], dim=-1)  # (B, L)                sv = torch.cat([sv, sv_i], dim=-1)     # (B, L)                dTS = torch.cat([dTS, dTS_i], dim=-1)  # (B, L)                rnn_input_list.append(self.prep_inj_marks(vm_i))                rnn_input_list.append(self.prep_inj_marks(dVc_i))                rnn_input_list.append(self.prep_inj_marks(sv_i))                rnn_input_list.append(self.prep_inj_marks(dTS_i))                context_list.append(self.prep_inj_marks(vm_i))                context_list.append(self.prep_inj_marks(sv_i))                context_list.append(self.prep_inj_marks(dTS_i))            context_list.append(self.prep_mag_marks((not update_history) * 1 * ones))            # Update the history/context.            if update_history:                rnn_input = torch.cat(rnn_input_list, dim=-1)                current_history, current_hidden_state = self.rnn( rnn_input, current_hidden_state.contiguous() )            current_context = torch.cat([current_history]+context_list,dim=-1)            # Check if this is the final loop iteration.            with torch.no_grad():                reached = inter_times.sum(-1).min()                generated = reached >= t_end - t_start        # Remove/pad any samples going beyond the end time.        duration = t_end - t_start        unclipped_arrival_times = inter_times.cumsum(-1)  # (B, L)        padding_mask = unclipped_arrival_times > duration        inter_times = torch.masked_fill(inter_times, padding_mask, 0.0)        vm = torch.masked_fill(vm, padding_mask, 0.0)        dVc = torch.masked_fill(dVc, padding_mask, 0.0)        sv = torch.masked_fill(sv, padding_mask, 0.0)        dTS = torch.masked_fill(dTS, padding_mask, 0.0)        end_idx = (1 - padding_mask.long()).sum(-1)        last_surv_time = duration - inter_times.sum(-1)        inter_times[torch.arange(batch_size), end_idx] = last_surv_time        end_idx2 = len(samp_seq.inj_time)        # Return the randomly sampled sequences (or batch).        batch = BatchIS(            inter_times=inter_times,            arrival_times=inter_times.cumsum(-1),            t_start=torch.full([batch_size], t_start, device=self.device).float(),            t_end=torch.full([batch_size], t_end, device=self.device).float(),            t_nll_start=torch.full([batch_size], t_start, device=self.device).float(),            mask=padding_mask.float(),            start_idx=torch.zeros(batch_size, device=self.device).long(),            end_idx=end_idx,            end_idx2=torch.full([batch_size], end_idx2, device=self.device).long(),            inj_time=samp_seq.inj_time.unsqueeze(0).expand(batch_size,-1),            inj_rate=samp_seq.inj_rate.unsqueeze(0).expand(batch_size,-1),            inj_dvol=samp_seq.inj_dvol.unsqueeze(0).expand(batch_size,-1),            inj_sign=samp_seq.inj_sign.unsqueeze(0).expand(batch_size,-1),            inj_tsgn=samp_seq.inj_tsgn.unsqueeze(0).expand(batch_size,-1),            mag_completeness=torch.full([batch_size], Mc, device=self.device).long(),            mag=magnitudes,            vm=vm,            dVc=dVc,            sv=sv,            dTS=dTS,        )        if return_sequences:            return batch.to_list()        else:            return batch    def interpolate_injection(self, arrival_times, sequence):        """ Simple subroutine to interpolate injection marks."""        vm = torch.from_numpy( np.interp(arrival_times.numpy(), sequence.arrival_times.numpy(), sequence.vm.numpy() ))        dVc = torch.from_numpy(np.interp(arrival_times.numpy(), sequence.arrival_times.numpy(), sequence.dVc.numpy() ))        sv = torch.from_numpy(np.interp(arrival_times.numpy(), sequence.arrival_times.numpy(), sequence.sv.numpy() ))        dTS = torch.from_numpy(np.interp(arrival_times.numpy(), sequence.arrival_times.numpy(), sequence.dTS.numpy() ))        return vm.float(), dVc.float(), sv.float(), dTS.float()